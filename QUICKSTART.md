# Guide de d√©marrage rapide

## üöÄ D√©marrage en 5 minutes

### 1. Pr√©requis
- Docker Desktop install√© et en cours d'ex√©cution
- 8 Go de RAM disponible minimum
- 20 Go d'espace disque

### 2. Configuration

Cr√©er le fichier `.env` depuis le template :
```bash
cp .env.example .env
```

√âditer `.env` et ajouter votre cl√© API OpenWeather :
```bash
OPENWEATHER_API_KEY=votre_cle_api_ici
```

> üí° Obtenez une cl√© API gratuite sur https://openweathermap.org/api

### 3. D√©marrage

**Linux/Mac:**
```bash
chmod +x start.sh
./start.sh
```

**Windows (PowerShell):**
```powershell
.\start.ps1
```

**Ou manuellement:**
```bash
docker-compose up -d
```

### 4. Acc√©der aux interfaces

Une fois tous les services d√©marr√©s (environ 2 minutes) :

| Service | URL | Identifiants |
|---------|-----|--------------|
| **Airflow** | http://localhost:8080 | admin / admin |
| **MinIO** | http://localhost:9001 | minioadmin / minioadmin |
| **Kafka UI** | http://localhost:9021 | - |
| **Spark** | http://localhost:8081 | - |

### 5. Lancer le pipeline

#### Option A : Via Airflow (Recommand√©)

1. Ouvrir Airflow UI : http://localhost:8080
2. Se connecter avec `admin` / `admin`
3. Activer le DAG `weather_pipeline`
4. Cliquer sur "Trigger DAG"

#### Option B : Manuellement

```bash
# 1. Ingestion des donn√©es
docker-compose exec airflow-webserver python /opt/airflow/src/ingestion/weather_producer.py

# 2. Consumer Kafka vers S3
docker-compose exec airflow-webserver python /opt/airflow/src/ingestion/kafka_consumer_s3.py

# 3. Processing Silver
docker-compose exec airflow-webserver python /opt/airflow/src/processing/silver_processor.py

# 4. Chargement Gold
docker-compose exec airflow-webserver python /opt/airflow/src/loading/gold_loader.py
```

### 6. V√©rifier les donn√©es

**PostgreSQL:**
```bash
docker-compose exec postgres psql -U dataeng -d weather_dw -c "SELECT COUNT(*) FROM gold_weather_detailed;"
```

**MinIO:**
Ouvrir http://localhost:9001 et naviguer dans les buckets `bronze-layer` et `silver-layer`

### 7. Visualiser dans Power BI

1. Ouvrir Power BI Desktop
2. Obtenir des donn√©es ‚Üí PostgreSQL
3. Serveur : `localhost:5432`
4. Base de donn√©es : `weather_dw`
5. S√©lectionner les tables :
   - `gold_weather_detailed`
   - `gold_weather_daily_aggregates`
   - `gold_weather_hourly_aggregates`

## üîß D√©pannage

### Les services ne d√©marrent pas

```bash
# V√©rifier les logs
docker-compose logs -f

# Red√©marrer un service sp√©cifique
docker-compose restart [service_name]
```

### Probl√®me de connexion API

```bash
# Tester manuellement
curl "https://api.openweathermap.org/data/2.5/weather?q=Paris&appid=VOTRE_CLE"
```

### R√©initialiser compl√®tement

```bash
# Arr√™ter et supprimer tout
docker-compose down -v

# Red√©marrer
./start.sh
```

## üìä Tests

```bash
# Tests unitaires
docker-compose exec airflow-webserver pytest tests/test_pipeline.py -v

# Tests d'int√©gration
docker-compose exec airflow-webserver python tests/test_integration.py
```

## üõë Arr√™t

```bash
# Arr√™ter tous les services
docker-compose down

# Arr√™ter et supprimer les donn√©es
docker-compose down -v
```

## üìù Configuration avanc√©e

### Changer les villes surveill√©es

√âditer `config/config.yaml` :
```yaml
ingestion:
  cities:
    - Paris
    - London
    - New York
    - Tokyo
    - Votre Ville
```

### Changer la fr√©quence d'ex√©cution

√âditer `airflow/dags/weather_pipeline_dag.py` :
```python
schedule_interval='0 * * * *',  # Chaque heure
# Ou
schedule_interval='*/30 * * * *',  # Toutes les 30 minutes
```

## üÜò Support

Pour toute question ou probl√®me :
1. V√©rifier les logs : `docker-compose logs -f [service]`
2. Consulter la documentation compl√®te dans `README.md`
3. V√©rifier les issues GitHub

## üéØ Prochaines √©tapes

1. Personnaliser les transformations dans `src/processing/silver_processor.py`
2. Ajouter des alertes dans Airflow
3. Cr√©er des dashboards Power BI
4. Impl√©menter des mod√®les ML sur les donn√©es historiques
